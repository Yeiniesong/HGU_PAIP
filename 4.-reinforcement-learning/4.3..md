# 4.3. AWS DeepRacer

![Figure 35\(a\). Structure of RL](../.gitbook/assets/figure-35-a-%20%281%29.png)

![Figure 35\(b\). Environment and Reward](../.gitbook/assets/figure-35-b-%20%281%29.png)

**Figure 35\(a\).**의 자동차는 세계 최대의 클라우드 서비스 기업 아마존웹서비스\(AWS: Amazon Web Service, 이하 AWS\)에서 2018년 11출시한 강화 학습 기반 자율주행차 ‘딥레이서\(DeepRacer\)’이다. **Figure 35\(a\).** 는 딥레이서와 트랙이 강화 학습을 통해 각각 Agent 와 Enviroment로 상호작용하는 과정을 보여준다. 강화 학습에서 Agent는 Total reward를 극대화하려는 목표를 가지고 환경과 상호 작용한다. 딥레이서의 강화학습 신경망\(Agent\)은 카메라로부터 얻은 전방 snap shot\(State\)에 따라 조향각과 속도를 선택하여 이동한다\(Action\). 이후 **Figure 36\(b\).**와 같이 트랙을 바탕으로 정의된 환경\(Environment\)은 보상\(reward\)과 다음 State를 반환한다. 더 디테일한 흐름은 여기서 잘 설명해주고 있으니, 자세한 설명은 생략하겠다

{% embed url="https://d2k9g1efyej86q.cloudfront.net/" %}

![Figure\(a\). Model Creation and  Training](../.gitbook/assets/figure-36-a-%20%281%29.png)

![Figure 36\(b\). Model Evaluation and Tuning](../.gitbook/assets/figure-36-b-%20%281%29.png)

딥레이서의 클라우드 기반 시뮬레이터 및 강화 학습 플랫폼을 이용하여 **Figure 36\(a\).** 와 같이 주행에 필요한 강화 학습 모델을 설계, 학습시키고, **Figure 36\(b\).**와 같이 딥레이서 시뮬레이터를 사용하여 설계한 모델을 평가, 튜닝 등의 작업이 가능하다. 또한 강화학습 모델을 실제 딥레이서에 탑재시켜 오프라인에서 물리적 트랙에서 테스트할 수 있으며 실제 자동차의 1/18 정도 비율로 제작되어 제법 큰 규모의 경주용 트랙이 필요하다. 트랙 제작에 제약이 많아 본 프로젝트에서는 캠퍼스 내 존재하는 자율주행 트랙을 가지고 실험하였다. 하지만 온라인으로 가상 대회에 참가할 수 있고, 실제 트랙의 템플릿을 축소하여 제작하여도 곧잘 동작한 사례가 있으니, 시도해보아도 좋을 것 같다.

{% embed url="https://medium.com/@babu.b.srinivasan/aws-deepracer-how-to-build-your-own-race-track-at-home-9fa74fea2ca9" %}



딥레이서에 탑재되는 강화학습 알고리즘은 기본적으로 PPO를 적용하고 있지만, AWS의 머신러닝 개발도구인 ‘세이지메이커\(SageMaker\)’를 이용하여 누구나 개발 가능하다.

PPO 알고리즘은 데이터 효율성 면에 있어서 큰 장점이 있으며 최근 그 성능을 인정 받고 있다. 알고리즘 개발/개선에 대해서는 본 수업의 궁극적인 목적이 아니니 PPO에 대해 간단하게 이해하기 쉬운 자료를 소개하고 넘어가겠다.

{% embed url="https://www.youtube.com/watch?v=cIyXYYdZIsk&list=PL\_iJu012NOxehE8fdF9me4TLfbdv3ZW8g&index=28&ab\_channel=%ED%98%81%ED%8E%9C%ED%95%98%EC%9E%84" %}



